Big O Asymptotic notation is a term that is often tossed 
around when it comes to algorithm analysis. 
It is also known as the Bachmann-Landau notation, named after Paul Bachmann and Edmond Landau, 
who are credited with inventing it.
You will also see Big O (big-oh) notation mentioned in the same context. 
Big O is a part of asymptotic notation. 
In short, Big O is a mathematical notation used in algorithm analysis to aid in 
determining what is known as the "limiting behavior" 
of the function that applies to your algorithm. 
It is used to classify algorithms according to limits on runtime or input size. 
In other words, it can tell you when an algorithm is not acceptable 
because its implementation will take too long to complete 
or it is not capable of handling the input size efficiently.
Big O is often represented as a graph that shows the growth rate of a function or algorithm. 
The big-Oh notation to describe the performance of searching an unsorted array 
would be denoted as O(n). 
The large script O is where the terminology big-Oh notation comes from, 
and the n indicates that the number of steps required to search an array grows linearly 
as the size of the array grows. 
O(n), or linear-time, represents just one of a myriad of possible asymptotic running times. 
Others include O(log2 n), O(n log2 n), O(n2), O(2n), and so on.
To compare two functions for determining which is better to use based on input size, 
you can find the two functions on the graph and locate the point at which they intersect